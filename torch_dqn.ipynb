{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/d3sm0/torch_dqn/blob/master/torch_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "320Z1BXmncPF"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhW2HUYPo354"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple,deque\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhW2HUYPo354"
   },
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, obs_shape, act_shape):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc_0 = torch.nn.Linear(obs_shape, 64)\n",
    "        self.fc_1 = torch.nn.Linear(64, 64)\n",
    "        self.out = torch.nn.Linear(64, act_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view((x.size(0), -1))\n",
    "        x = torch.nn.functional.relu(self.fc_0(x))\n",
    "        x = torch.nn.functional.relu(self.fc_1(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZ78Dq0qo-Ox"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qdz60hHvpCVR"
   },
   "outputs": [],
   "source": [
    "class LinearSchedule(object):\n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
    "        \"\"\"Linear interpolation between initial_p and final_p over\n",
    "        schedule_timesteps. After this many timesteps pass final_p is\n",
    "        returned.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        schedule_timesteps: int\n",
    "            Number of timesteps for which to linearly anneal initial_p\n",
    "            to final_p\n",
    "        initial_p: float\n",
    "            initial output value\n",
    "        final_p: float\n",
    "            final output value\n",
    "        \"\"\"\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.final_p = final_p\n",
    "        self.initial_p = initial_p\n",
    "        self.p = self.initial_p\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.p\n",
    "\n",
    "    def reset(self):\n",
    "        self.p = self.initial_p\n",
    "\n",
    "    def update(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        if self.p > self.final_p:\n",
    "            fraction = min(float(t) / self.schedule_timesteps, 1.0)\n",
    "            self.p = self.initial_p + fraction * (self.final_p - self.initial_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "max_steps = int(1e5)\n",
    "env = 'CartPole-v1'\n",
    "gamma = .99\n",
    "train_every = 4\n",
    "update_every int(1e4)\n",
    "batch_siz = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "J-2gTk3UpGjE",
    "outputId": "7ea069c9-91cb-426e-d2f3-a9a5d02c9d0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# env = ImgObsWrapper(env)\n",
    "#path = args.base_path + args.env\n",
    "#os.makedirs(path, exist_ok=True)\n",
    "obs_shape = env.observation_space.shape[0]\n",
    "act_shape = env.action_space.n\n",
    "\n",
    "q = QNetwork(obs_shape, act_shape)\n",
    "q_target = QNetwork(obs_shape, act_shape)\n",
    "opt = optim.Adam(lr=1e-3, params=q.parameters())\n",
    "memory = Memory(capacity=int(1e4))\n",
    "scheduler = LinearSchedule(schedule_timesteps=int(1e5 * 0.1), final_p=0.01)\n",
    "\n",
    "\n",
    "avg_rw = deque(maxlen=40)\n",
    "avg_len = deque(maxlen=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZYfHc1_pMWI"
   },
   "outputs": [],
   "source": [
    "def get_action(s, t):\n",
    "\n",
    "    s = torch.Tensor(s[None, :])\n",
    "    _q = q(s)\n",
    "    if np.random.sample() > scheduler.value:\n",
    "        best_action = np.argmax(_q.detach(), axis=-1).item()\n",
    "    else:\n",
    "        best_action = np.random.randint(0, act_shape)\n",
    "        scheduler.update(t)\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOxnjCGRpWcu"
   },
   "outputs": [],
   "source": [
    " def train(batch, gamma = .99):\n",
    "        batch = Transition(*zip(*batch))\n",
    "        s = torch.Tensor(batch.state)\n",
    "        a = torch.Tensor(one_hot(np.array(batch.action), num_classes=act_shape))\n",
    "        r = torch.Tensor(batch.reward)\n",
    "        d = torch.Tensor(batch.done)\n",
    "        s1 = torch.Tensor(batch.next_state)\n",
    "\n",
    "        value = (q(s) * a).sum(dim=-1)\n",
    "        next_value = r + gamma* (1. - d) * torch.max(q_target(s1), dim=-1)[0]\n",
    "        loss = (.5 * (next_value - value) ** 2).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "ZfUdA6vkptze",
    "outputId": "ca31155a-3992-4d31-8f96-e1b7191bfd81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:0\tep:0\tavg_rw:12.666666666666666\tavg_len:12.666666666666666\teps:1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4d5d5f4d4f3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m    \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m    \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-a1061f9ab9e0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(batch, gamma)\u001b[0m\n\u001b[1;32m     12\u001b[0m        \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m        \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m        \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "\n",
    "ep_rw = 0\n",
    "ep_len = 0\n",
    "ep = 0\n",
    "for t in range(int(max_steps)):\n",
    "    action = get_action(state, t)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.push(state, action, next_state, reward, done)\n",
    "    ep_rw += reward\n",
    "    ep_len += 1\n",
    "\n",
    "    state = next_state.copy()\n",
    "    if done:\n",
    "        ep += 1\n",
    "        avg_rw.append(ep_rw)\n",
    "        avg_len.append(ep_len)\n",
    "        ep_rw = 0\n",
    "        ep_len = 0\n",
    "        state = env.reset()\n",
    "\n",
    "    if t % train_every == 0 and len(memory) > batch_size:\n",
    "        batch = memory.sample(batch_size=batch_size)\n",
    "        train(batch)\n",
    "\n",
    "    if t % update_every == 0:\n",
    "        q_target.load_state_dict(q.state_dict())\n",
    "        print(f't:{t}\\tep:{ep}\\tavg_rw:{np.mean(avg_rw)}\\tavg_len:{np.mean(avg_len)}\\teps:{scheduler.value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhqnkrofpxPD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "torch_dqn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
